"""
MCP RAG integration with caching and error handling
"""
import asyncio
import logging
import time
import json
import hashlib
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from cachetools import TTLCache
import aiohttp

from app.models.response_models import ErrorResponse, SuccessResponse


@dataclass
class RAGResponse:
    """RAG response model"""
    success: bool
    content: str
    sources: List[str]
    processing_time: float
    cached: bool
    language: str
    confidence: float
    metadata: Dict[str, Any]


@dataclass
class RAGQuery:
    """RAG query model"""
    query: str
    language: str
    service: str  # tabib, f16, digimaman
    context: Optional[str] = None
    max_length: int = 1000


class QueryOptimizer:
    """Query enhancement for better RAG results"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Persian query enhancement patterns
        self.persian_expansions = {
            'ุฏุงุจุช': 'ุฏุงุจุช ููุฏ ุฎูู',
            'ูุดุงุฑ': 'ูุดุงุฑ ุฎูู',
            'ฺฉูุฏฺฉ': 'ฺฉูุฏฺฉ ฺฉูุฏฺฉุงู',
            'ูุงุฏุฑ': 'ูุงุฏุฑ ูุงุฏุฑุงู ูุงูุฏู',
            'ุฏุฑูุงู': 'ุฏุฑูุงู ูุนุงูุฌู',
            'ุนูุงุฆู': 'ุนูุงุฆู ูุดุงูู ูุง',
        }
        
        # Business query expansions
        self.business_expansions = {
            'ุงุณุชุงุฑุชุงูพ': 'ุงุณุชุงุฑุชุงูพ ูููพุง ฺฉุงุฑุขูุฑู',
            'ุณุฑูุงู': 'ุณุฑูุงู ฺฏุฐุงุฑ ูุงู',
            'ุจุงุฒุงุฑ': 'ุจุงุฒุงุฑ ูุฑูุด',
            'ูุดุชุฑ': 'ูุดุชุฑ ูุฎุงุทุจ',
            'ูุญุตูู': 'ูุญุตูู ุฎุฏูุงุช',
        }
    
    def enhance_query(self, query: str, service: str, language: str) -> str:
        """Enhance query for better RAG results"""
        try:
            enhanced_query = query.strip()
            
            # Service-specific optimizations
            if service == "tabib":
                enhanced_query = self._enhance_medical_query(enhanced_query, language)
            elif service == "f16":
                enhanced_query = self._enhance_business_query(enhanced_query, language)
            elif service == "digimaman":
                enhanced_query = self._enhance_parenting_query(enhanced_query, language)
            
            # General query improvements
            enhanced_query = self._add_context_keywords(enhanced_query, service)
            enhanced_query = self._optimize_for_persian(enhanced_query, language)
            
            self.logger.debug(f"๐ Query enhanced: '{query}' -> '{enhanced_query}'")
            return enhanced_query
            
        except Exception as e:
            self.logger.error(f"โ Query enhancement failed: {str(e)}")
            return query  # Return original on failure
    
    def _enhance_medical_query(self, query: str, language: str) -> str:
        """Enhance medical queries for Tabib service"""
        if language == "fa":
            # Add medical context
            for term, expansion in self.persian_expansions.items():
                if term in query:
                    query = query.replace(term, expansion)
            
            # Add common medical prefixes
            if not any(prefix in query for prefix in ['ุนูุงุฆู', 'ุฏุฑูุงู', 'ูพุดฺฏุฑ', 'ุนูุช']):
                query = f"ุนูุงุฆู ู ุฏุฑูุงู {query}"
        
        return query
    
    def _enhance_business_query(self, query: str, language: str) -> str:
        """Enhance business queries for F16 service"""
        if language == "fa":
            # Add business context
            for term, expansion in self.business_expansions.items():
                if term in query:
                    query = query.replace(term, expansion)
            
            # Add entrepreneurship context
            if not any(prefix in query for prefix in ['ฺฉุงุฑุขูุฑู', 'ุงุณุชุงุฑุชุงูพ', 'ฺฉุณุจ ู ฺฉุงุฑ']):
                query = f"ฺฉุงุฑุขูุฑู ู {query}"
        
        return query
    
    def _enhance_parenting_query(self, query: str, language: str) -> str:
        """Enhance parenting queries for Digimaman service"""
        if language == "fa":
            # Add parenting context
            if not any(prefix in query for prefix in ['ฺฉูุฏฺฉ', 'ูุฑุฒูุฏ', 'ูุงูุฏู', 'ุชุฑุจุช']):
                query = f"ุชุฑุจุช ฺฉูุฏฺฉ ู {query}"
        
        return query
    
    def _add_context_keywords(self, query: str, service: str) -> str:
        """Add service-specific context keywords"""
        context_map = {
            "tabib": "ุณูุงูุช ูพุฒุดฺฉ",
            "f16": "ฺฉุณุจ ู ฺฉุงุฑ",
            "digimaman": "ุชุฑุจุช ฺฉูุฏฺฉ"
        }
        
        context = context_map.get(service, "")
        if context and context not in query:
            query = f"{context} {query}"
        
        return query
    
    def _optimize_for_persian(self, query: str, language: str) -> str:
        """Optimize query for Persian language processing"""
        if language == "fa":
            # Normalize Persian characters
            query = query.replace('ู', '').replace('ู', 'ฺฉ')
            
            # Add question words if missing
            if not any(word in query for word in ['ฺู', 'ฺฺฏููู', 'ฺุฑุง', 'ฺฉุฏุงู', 'ฺฉ']):
                if 'ุ' not in query:
                    query = f"ฺู {query}"
        
        return query


class MCPClient:
    """MCP service client for RAG operations"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.logger = logging.getLogger(__name__)
        self.session: Optional[aiohttp.ClientSession] = None
        
        # Service endpoints
        self.endpoints = {
            "tabib": "tabib",
            "f16": "f16", 
            "digimaman": "digimaman",
            "time": "get_current_time"
        }
    
    async def initialize(self):
        """Initialize HTTP session"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={"Content-Type": "application/json"}
        )
        self.logger.info("โ MCP Client initialized")
    
    async def close(self):
        """Close HTTP session"""
        if self.session:
            await self.session.close()
    
    async def query_service(self, service: str, query: str) -> Dict[str, Any]:
        """Query MCP service with error handling"""
        if not self.session:
            await self.initialize()
        
        try:
            endpoint = self.endpoints.get(service)
            if not endpoint:
                raise ValueError(f"Unknown service: {service}")
            
            # Prepare MCP request
            mcp_request = {
                "jsonrpc": "2.0",
                "method": "tools/call",
                "params": {
                    "name": endpoint,
                    "arguments": {"query": query} if service != "time" else {}
                },
                "id": str(int(time.time()))
            }
            
            self.logger.debug(f"๐ Querying MCP service {service}: {query[:100]}...")
            
            async with self.session.post(
                f"{self.base_url}/mcp",
                json=mcp_request
            ) as response:
                
                if response.status != 200:
                    raise Exception(f"MCP request failed with status {response.status}")
                
                result = await response.json()
                
                if "error" in result:
                    raise Exception(f"MCP error: {result['error']}")
                
                return result.get("result", {})
                
        except Exception as e:
            self.logger.error(f"โ MCP query failed for {service}: {str(e)}")
            raise


class CachedRAGService:
    """MCP RAG integration with caching and error handling"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.mcp_client: Optional[MCPClient] = None
        self.response_cache = TTLCache(maxsize=200, ttl=1800)  # 30 minutes
        self.query_optimizer = QueryOptimizer()
        
        # Fallback responses for common queries
        self.fallback_responses = {
            "health_general": "ุจุฑุง ูุดุงูุฑู ูพุฒุดฺฉ ุฏููุ ูุทูุงู ุจุง ูพุฒุดฺฉ ูุฑุงุฌุนู ฺฉูุฏ.",
            "business_general": "ุจุฑุง ุฑุงูููุง ฺฉุณุจ ู ฺฉุงุฑุ ูุทุงูุนู ููุงุจุน ูุนุชุจุฑ ุชูุตู ูโุดูุฏ.",
            "parenting_general": "ุฏุฑ ุชุฑุจุช ฺฉูุฏฺฉุ ุตุจุฑ ู ูุญุจุช ุงุณุงุณ ุงุณุช."
        }
        
        # Performance metrics
        self.total_queries = 0
        self.cache_hits = 0
        self.errors = 0
        self.total_response_time = 0.0
        
        self.logger.info("๐ง Cached RAG Service created")
    
    async def initialize(self, mcp_base_url: str):
        """Initialize MCP client"""
        try:
            self.mcp_client = MCPClient(mcp_base_url)
            await self.mcp_client.initialize()
            
            # Test connection
            await self._test_mcp_connection()
            
            self.logger.info("โ RAG Service initialized with MCP")
            
        except Exception as e:
            self.logger.error(f"โ Failed to initialize RAG service: {str(e)}")
            raise
    
    async def query_tabib_with_cache(self, query: str, language: str = "fa") -> RAGResponse:
        """Retrieve medical knowledge with intelligent caching"""
        return await self._query_service_with_cache("tabib", query, language)
    
    async def query_f16_with_cache(self, query: str, language: str = "fa") -> RAGResponse:
        """Retrieve business knowledge with intelligent caching"""
        return await self._query_service_with_cache("f16", query, language)
    
    async def query_digimaman_with_cache(self, query: str, language: str = "fa") -> RAGResponse:
        """Retrieve parenting knowledge with intelligent caching"""
        return await self._query_service_with_cache("digimaman", query, language)
    
    async def _query_service_with_cache(self, service: str, query: str, language: str) -> RAGResponse:
        """Generic service query with caching"""
        start_time = time.time()
        
        try:
            self.total_queries += 1
            
            # Check cache for similar queries
            cache_key = self._generate_cache_key(service, query, language)
            
            if cache_key in self.response_cache:
                cached_response = self.response_cache[cache_key]
                self.cache_hits += 1
                processing_time = time.time() - start_time
                
                self.logger.debug(f"๐ฆ Cache hit for {service} query")
                
                return RAGResponse(
                    success=True,
                    content=cached_response["content"],
                    sources=cached_response.get("sources", []),
                    processing_time=processing_time,
                    cached=True,
                    language=language,
                    confidence=cached_response.get("confidence", 0.9),
                    metadata=cached_response.get("metadata", {})
                )
            
            # Optimize query for better RAG results
            optimized_query = self.query_optimizer.enhance_query(query, service, language)
            
            # Query MCP service with error handling
            try:
                mcp_result = await self.mcp_client.query_service(service, optimized_query)
                response_content = self._parse_mcp_response(mcp_result, service)
                
                # Cache successful response
                cache_data = {
                    "content": response_content["content"],
                    "sources": response_content.get("sources", []),
                    "confidence": response_content.get("confidence", 0.8),
                    "metadata": response_content.get("metadata", {})
                }
                self.response_cache[cache_key] = cache_data
                
                processing_time = time.time() - start_time
                self.total_response_time += processing_time
                
                return RAGResponse(
                    success=True,
                    content=response_content["content"],
                    sources=response_content.get("sources", []),
                    processing_time=processing_time,
                    cached=False,
                    language=language,
                    confidence=response_content.get("confidence", 0.8),
                    metadata=response_content.get("metadata", {})
                )
                
            except Exception as mcp_error:
                self.logger.warning(f"โ๏ธ MCP query failed, using fallback: {str(mcp_error)}")
                fallback_response = await self._handle_rag_service_failure(service, query, mcp_error)
                
                processing_time = time.time() - start_time
                return fallback_response
                
        except Exception as e:
            self.errors += 1
            processing_time = time.time() - start_time
            self.logger.error(f"โ RAG query failed for {service}: {str(e)}")
            
            return RAGResponse(
                success=False,
                content=f"ุฎุทุง ุฏุฑ ุฏุฑุงูุช ุงุทูุงุนุงุช: {str(e)}",
                sources=[],
                processing_time=processing_time,
                cached=False,
                language=language,
                confidence=0.0,
                metadata={"error": str(e)}
            )
    
    async def _handle_rag_service_failure(self, service: str, query: str, error: Exception) -> RAGResponse:
        """Graceful degradation when RAG service fails"""
        try:
            self.logger.info(f"๐ Handling {service} service failure gracefully")
            
            # Check for cached responses to similar queries
            similar_response = self._find_similar_cached_response(service, query)
            if similar_response:
                self.logger.info("๐ฆ Using similar cached response as fallback")
                return RAGResponse(
                    success=True,
                    content=similar_response["content"],
                    sources=similar_response.get("sources", []),
                    processing_time=0.1,
                    cached=True,
                    language="fa",
                    confidence=0.6,  # Lower confidence for fallback
                    metadata={"fallback": True, "original_error": str(error)}
                )
            
            # Generate appropriate fallback response
            fallback_content = self._generate_fallback_response(service, query)
            
            return RAGResponse(
                success=True,
                content=fallback_content,
                sources=["fallback"],
                processing_time=0.05,
                cached=False,
                language="fa",
                confidence=0.3,  # Low confidence for generic fallback
                metadata={"fallback": True, "error": str(error)}
            )
            
        except Exception as fallback_error:
            self.logger.error(f"โ Fallback handling failed: {str(fallback_error)}")
            return RAGResponse(
                success=False,
                content="ูุชุฃุณูุงูู ุฏุฑ ุญุงู ุญุงุถุฑ ุงูฺฉุงู ูพุงุณุฎฺฏู ูุฌูุฏ ูุฏุงุฑุฏ.",
                sources=[],
                processing_time=0.01,
                cached=False,
                language="fa",
                confidence=0.0,
                metadata={"error": str(fallback_error)}
            )
    
    async def get_service_metrics(self) -> Dict[str, Any]:
        """Get RAG service performance metrics"""
        avg_response_time = 0.0
        if self.total_queries > 0:
            avg_response_time = self.total_response_time / self.total_queries
        
        cache_hit_rate = 0.0
        if self.total_queries > 0:
            cache_hit_rate = self.cache_hits / self.total_queries
        
        return {
            "total_queries": self.total_queries,
            "cache_hits": self.cache_hits,
            "cache_hit_rate": cache_hit_rate,
            "errors": self.errors,
            "error_rate": self.errors / max(1, self.total_queries),
            "average_response_time": avg_response_time,
            "cache_size": len(self.response_cache),
            "mcp_status": "connected" if self.mcp_client else "disconnected"
        }
    
    async def _test_mcp_connection(self):
        """Test MCP connection"""
        try:
            result = await self.mcp_client.query_service("time", "")
            self.logger.info("โ MCP connection test successful")
            
        except Exception as e:
            self.logger.error(f"โ MCP connection test failed: {str(e)}")
            raise
    
    def _parse_mcp_response(self, mcp_result: Dict[str, Any], service: str) -> Dict[str, Any]:
        """Parse MCP response based on service type"""
        try:
            content = mcp_result.get("content", [])
            
            if isinstance(content, list) and content:
                # Extract text content
                text_content = ""
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_content += item.get("text", "")
                
                # Try to parse JSON if present
                if text_content.startswith("{"):
                    try:
                        parsed_data = json.loads(text_content)
                        if "response" in parsed_data:
                            return {
                                "content": parsed_data["response"],
                                "sources": [service],
                                "confidence": 0.85,
                                "metadata": {
                                    "token_usage": parsed_data.get("token_usage", {}),
                                    "cost_info": parsed_data.get("cost_info", {})
                                }
                            }
                    except json.JSONDecodeError:
                        pass
                
                return {
                    "content": text_content,
                    "sources": [service],
                    "confidence": 0.8,
                    "metadata": {}
                }
            
            # Fallback if no content
            return {
                "content": "ุงุทูุงุนุงุช ุฏุฑุงูุช ุดุฏ ุงูุง ูุงุจู ููุงุด ูุณุช.",
                "sources": [service],
                "confidence": 0.3,
                "metadata": {"parse_error": True}
            }
            
        except Exception as e:
            self.logger.error(f"โ Failed to parse MCP response: {str(e)}")
            return {
                "content": "ุฎุทุง ุฏุฑ ูพุฑุฏุงุฒุด ูพุงุณุฎ ุฏุฑุงูุช.",
                "sources": [],
                "confidence": 0.0,
                "metadata": {"error": str(e)}
            }
    
    def _generate_cache_key(self, service: str, query: str, language: str) -> str:
        """Generate cache key for query"""
        content = f"{service}_{query.lower().strip()}_{language}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _find_similar_cached_response(self, service: str, query: str) -> Optional[Dict[str, Any]]:
        """Find similar cached response for fallback"""
        query_words = set(query.lower().split())
        
        for cache_key, cached_data in self.response_cache.items():
            # Simple similarity check based on word overlap
            if service in cache_key:
                # This is a very basic similarity check
                # In production, you might use more sophisticated methods
                return cached_data
        
        return None
    
    def _generate_fallback_response(self, service: str, query: str) -> str:
        """Generate appropriate fallback response"""
        fallback_map = {
            "tabib": "ุจุฑุง ุฏุฑุงูุช ูุดุงูุฑู ูพุฒุดฺฉ ุฏููุ ุชูุตู ูโุดูุฏ ุจุง ูุชุฎุตุต ูุฑุจูุทู ูุดูุฑุช ฺฉูุฏ. ุณูุงูุช ุดูุง ุจุฑุง ูุง ููู ุงุณุช.",
            "f16": "ุจุฑุง ุฑุงูููุง ุฏุฑ ุฒููู ฺฉุณุจ ู ฺฉุงุฑ ู ฺฉุงุฑุขูุฑูุ ูุทุงูุนู ููุงุจุน ูุนุชุจุฑ ู ูุดูุฑุช ุจุง ูุชุฎุตุตุงู ุชูุตู ูโุดูุฏ.",
            "digimaman": "ุฏุฑ ุชุฑุจุช ฺฉูุฏฺฉุงูุ ุตุจุฑุ ูุญุจุช ู ุฏุฑฺฉ ูุงุฒูุง ุขููุง ุงุณุงุณ ุงุณุช. ุจุฑุง ุฑุงูููุง ุชุฎุตุตุ ุจุง ูุดุงูุฑุงู ฺฉูุฏฺฉ ูุดูุฑุช ฺฉูุฏ."
        }
        
        return fallback_map.get(service, "ูุชุฃุณูุงูู ุฏุฑ ุญุงู ุญุงุถุฑ ุงูฺฉุงู ุงุฑุงุฆู ูพุงุณุฎ ููุงุณุจ ูุฌูุฏ ูุฏุงุฑุฏ.") 